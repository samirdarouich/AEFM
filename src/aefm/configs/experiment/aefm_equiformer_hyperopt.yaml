# @package _global_
defaults:
  - aefm_equiformer
  - override /callbacks:
      - checkpoint
      - lrmonitor
      - ema
      - earlystopping

# Define this flag so test set is not evaluated during hyperoptimization
hyperopt: True

run:
  id: aefm_hyper_equiformer

trainer:
  max_epochs: 2000

globals:
  # Parameters changed in hyperoptimization
  lr: 1.0e-4
  weight_decay: 1.0e-3
  bond_loss_cutoff: 2.0
  bond_loss_weight: 0.5
  sigma_fm: 0.05
  num_layers: 12
  n_atom_basis: 128
  ffn_hidden_channels: 512
  lmax: 6

  # Flow process to define x_t and target v_t
  flow_process:
    _target_: aefm.processes.VFM
    sigma: ${globals.sigma_fm}
    invariant: True 
    dtype: float64

# This defines the backbone model.
model:
  input_modules:
    - _target_: aefm.transform.Convert2PyG # Converts schnet inputs to PyG data
  
  representation:
    _target_: aefm.representation.equiformer_v2.equiformer_v2_denoising.EquiformerV2S_OC20_DenoisingPos
    output_key: ${globals.target_output_key}
    max_radius: ${globals.cutoff}
    num_layers: ${globals.num_layers}
    sphere_channels: ${globals.n_atom_basis}
    num_sphere_samples: ${globals.n_atom_basis}
    edge_channels: ${globals.n_atom_basis}
    ffn_hidden_channels: ${globals.ffn_hidden_channels}
    lmax_list: ["${globals.lmax}"]
    mmax_list: [2]

callbacks:
  early_stopping:
   patience: 100

data:

  # if hyperparameters should be tuned, use oa_reactdiff_split_own, which exlucdes 1000 
  # training samples for validation
  split_file: ${run.data_dir}/splits/oa_reactdiff_split_own.npz 

  num_train: 8000
  num_val: 1000
  num_test: 1073

# This defines the training task.
task:
  skip_exploding_batches: True
  outputs:
    # Computes the MSE loss between the predicted and target positions
    - _target_: schnetpack.task.ModelOutput
      name: ${globals.target_output_key}
      target_property: ${globals.target_key}
      loss_fn:
        _target_: torch.nn.MSELoss
      metrics:
        mse:
          _target_: torchmetrics.regression.MeanSquaredError
      loss_weight: 1.0
    
    # Computes the physical bond loss to avoid artefacts in generative models
    - _target_: aefm.task.BondModelOutput
      name: ${globals.target_output_key}
      target_property: ${globals.target_positions_key}
      cutoff: ${globals.bond_loss_cutoff}
      loss_fn:
        _target_: torch.nn.MSELoss
      metrics:
        mse_bond:
          _target_: torchmetrics.regression.MeanSquaredError
      loss_weight: ${globals.bond_loss_weight}

  # Regularization
  optimizer_args:
    weight_decay: ${globals.weight_decay}